{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os, sys, math, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, time, glob, os, sys\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# customized ftns \n",
    "from helpers import *\n",
    "from utilities import *\n",
    "from model import *\n",
    "from train import train_all_single_policies\n",
    "# ---------------------------------------------------------\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "# ---------------------------------------------------------\n",
    "# directories\n",
    "main_dir = '../'\n",
    "game_dir = main_dir+'data/'\n",
    "Data = LoadData(main_dir, game_dir)\n",
    "models_path = './models/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "game_id = '0021500463'\n",
    "game_data = Data.load_game(game_id)\n",
    "events_df = pd.DataFrame(game_data['events'])\n",
    "print('raw events shape:', events_df.shape)\n",
    "events_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get some suplementary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# play id to play roles/positions\n",
    "id_role = id_position(events_df)\n",
    "check_game_roles_duplicates(id_role)\n",
    "\n",
    "# its possible that F has similar role as G-f or F-G, we create empty slots to ensure meta order\n",
    "# ddentify defending and offending runs (this is included in process_moments)\n",
    "court_index = Data.load_csv('./meta_data/court_index.csv')\n",
    "court_index = dict(zip(court_index.game_id, court_index.court_position))\n",
    "\n",
    "# home and visitor ids\n",
    "homeid = events_df.loc[0].home['teamid']\n",
    "awayid = events_df.loc[0].visitor['teamid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process \n",
    "filter events, subsample frames, add velocity, reorder moments, re-arrange team order\n",
    "shot clock, filter out event with short moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out actions except 1: Make, 2: Miss, 4: Rebound, 6:Personal Foul, 7:Violation\n",
    "use_event = [1, 2, 4, 6, 7]\n",
    "discard_event = [3, 5, 8, 9, 10, 12, 13, 18]\n",
    "events = filter_event_type(events_df, discard_event)\n",
    "print('After filtering events has shape:', events.shape)\n",
    "# break up sequences at 24secs shot clock point (or irregular case, e.g. out of bound maybe),\n",
    "# and obtain the game data\n",
    "subsample_factor = 0\n",
    "single_game, single_game_balls = get_game_data_ra(events, court_index, game_id, \n",
    "                                                  event_threshold=10, subsample_factor=subsample_factor)\n",
    "print('Final number of events:', len(single_game))\n",
    "\n",
    "# get player velocity\n",
    "fs_base = 1./25 # 1/25 sec/frame   or  25 frames/sec\n",
    "fs = fs_base * subsample_factor if subsample_factor != 0 else fs_base\n",
    "single_game = [get_velocity(i, fs, mode=1) for i in single_game]\n",
    "n_events = len(single_game)\n",
    "\n",
    "# get basketball velocity\n",
    "single_game_balls = [np.concatenate([i[:-1, :], get_velocity(i, fs, mode=0)], axis=1) for i in single_game_balls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_game[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_game_balls[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Role assignment and reorder moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first prepare data\n",
    "n_defend = 5\n",
    "n_offend = 5\n",
    "n_ind = 4\n",
    "\n",
    "# length for each moment\n",
    "event_lengths = np.array([len(i) for i in single_game])\n",
    "# repeat the event_lengths 5 times in order to match the unstack later on with moments\n",
    "event_lengths_repeat = np.concatenate([event_lengths for _ in range(n_defend)], axis=0)\n",
    "# all the moments\n",
    "all_moments = np.concatenate(single_game, axis=0)\n",
    "all_moments_vel = np.concatenate(single_game, axis=0) # vel\n",
    "# we only need the first 5 players x,y coordindates\n",
    "# defend\n",
    "all_defend_moments = all_moments[:, :n_ind*n_defend]\n",
    "# offend\n",
    "all_offend_moments = all_moments[:, n_ind*n_offend:]\n",
    "\n",
    "# flattened\n",
    "all_defend_moments_ = np.concatenate([all_defend_moments[:, i:i+n_ind] for i in range(0, n_ind*n_defend, n_ind)], axis=0)\n",
    "all_offend_moments_ = np.concatenate([all_offend_moments[:, i:i+n_ind] for i in range(0, n_ind*n_offend, n_ind)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create hmm model\n",
    "n_comp = 7\n",
    "n_mix = None\n",
    "RA = RoleAssignment(n_iter=50,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "defend_state_sequence_, defend_means, defend_covs, _ = RA.train_hmm(all_defend_moments_, event_lengths_repeat, n_comp, n_mix)\n",
    "offend_state_sequence_, offend_means, offend_covs, _ = RA.train_hmm(all_offend_moments_, event_lengths_repeat, n_comp, n_mix)\n",
    "# get role orders\n",
    "_, defend_roles = RA.assign_roles(all_defend_moments_, all_defend_moments, defend_means, event_lengths)\n",
    "_, offend_roles = RA.assign_roles(all_offend_moments_, all_offend_moments, offend_means, event_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defend_pos_vel = order_moment_ra([i[:, :n_ind*5] for i in single_game], defend_roles)\n",
    "offend_pos_vel = order_moment_ra([i[:, n_ind*5:] for i in single_game], offend_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_game_balls[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defend_pos_vel[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defend_pos_vel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate defend, offend roles pos and velocity and the basketball pos and vel\n",
    "single_game = [np.concatenate([defend_pos_vel[i], offend_pos_vel[i], single_game_balls[i]], axis=1) for i in range(n_events)]\n",
    "# single_game = [np.concatenate([defend_pos_vel[i], offend_pos_vel[i]], axis=1) for i in range(n_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_game[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_roles = [np.concatenate([defend_roles[i], offend_roles[i]], axis=1) for i in range(len(single_game))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roles[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the plot, for the sake of comparison with processed moment later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot = PlotGame(game_id, main_dir, game_dir)\n",
    "# # for i in range(plotn): \n",
    "# Plot.load_moment2img(game_data, event_number=0, moment_number=0, return_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # manual plot check\n",
    "# plot_check(single_game, plt_ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build graph and starts training for all single policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = list(range(7))\n",
    "single_policies = [ImportGraph('policy{}/'.format(str(i))) for i in policies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = ImportGraph('policy1/')\n",
    "# result = model.run(train_x=val_xi, train_y=val_yi, seq_len=true_seq_len, h=k)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "overlap = 15\n",
    "batch_size = 32\n",
    "policy_targets = [np.array(get_sequences(single_game, policy, sequence_length, overlap)[1]) for policy in policies]\n",
    "policy_targets = np.concatenate(policy_targets, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy_targets[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(12).reshape(2,3,2)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = np.arange(10,22).reshape(2,3,2)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.concatenate((a,b), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_targets[0][4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At each epoch we will first do a forward pass to get the current step output, then we will use it for next roll-out position input and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "overlap = 15\n",
    "batch_size = 32\n",
    "policy_targets = [np.array(get_sequences(single_game, policy, sequence_length, overlap)[1]) for policy in policies]\n",
    "policy_targets = np.concatenate(policy_targets, axis=2)\n",
    "train = get_sequences(single_game, 0, sequence_length, overlap)[0] # all the train is same for all policy so we just choose one\n",
    "\n",
    "# starts training\n",
    "printn = 100    # how many epochs we print\n",
    "n_epoch = int(1e2)\n",
    "# look-ahead horizon\n",
    "horizon = [1] # we start from 1 as 0 is contained from single policy already\n",
    "t_int = time.time()\n",
    "train_step = 0\n",
    "valid_step = 0\n",
    "for k in horizon:\n",
    "    print('Horizon {0:} {1:}'.format(k, '='*10))\n",
    "    for epoch in range(n_epoch):\n",
    "        print('Epoch:', epoch)\n",
    "        epoch_loss =0.\n",
    "        # number of train batches\n",
    "        n_train_batch = len(train)//batch_size\n",
    "        t1 = time.time()\n",
    "        for batch in iterate_minibatches(train, policy_targets, batch_size, shuffle=False):\n",
    "            train_xi, train_yi = batch\n",
    "            train_pxi = []\n",
    "            # go through each policy\n",
    "            for policy in policies:\n",
    "                model = single_policies[policy]\n",
    "                # forward_pass make predictions for current timestep\n",
    "                p = model.forward_pass(train_xi, k)\n",
    "                # combine and recreate input\n",
    "                train_pxi.append(p[0])\n",
    "            # update each other\n",
    "            train_xi_updated = np.copy(train_xi)\n",
    "            for policy in policies:\n",
    "                for i in range(batch_size):\n",
    "                    # only updates based on horzion\n",
    "                    # how to deal with the the part where horzion can't reach????????????????????????????????????\n",
    "                    train_xi_updated[i][list(range(1, sequence_length-1, k+1)), policy*4:policy*4+2]\\\n",
    "                        = train_pxi[policy][i][list(range(0, sequence_length-2, k+1)), :]\n",
    "            # feed back into the model for training\n",
    "            for policy in policies:\n",
    "                train_pyi = [i[:, policy*2:policy*2+2] for i in train_yi]\n",
    "                p, l, _ = model.backward_pass(train_xi_updated, train_pyi, 0)\n",
    "            epoch_loss += l/n_train_batch\n",
    "            train_step += 1\n",
    "        # print out info\n",
    "        if epoch%printn ==0:\n",
    "#             # number of validation batches\n",
    "#             n_val_batch = len(test_game)//batch_size\n",
    "            t2 = time.time()\n",
    "#             valid_loss = 0\n",
    "#             for test_batch in iterate_minibatches(test_game, test_target, batch_size, shuffle=False):\n",
    "#                 val_xi, val_yi = test_batch\n",
    "#                 val_l, valid_sum = model.validate(val_xi, val_yi, k)\n",
    "\n",
    "#                 model.valid_writer.add_summary(valid_sum, valid_step)\n",
    "#                 valid_loss += val_l/n_val_batch\n",
    "#                 valid_step += printn\n",
    "#             print('Epoch {0:<4d} | loss: {1:<8.2f} | time took: {2:<.2f}s '\n",
    "#                 '| validation loss: {3:<8.2f}'.format(epoch, epoch_loss, (t2-t1), valid_loss))\n",
    "            print(epoch, epoch_loss, t2-t1)\n",
    "\n",
    "    print('Total time took: {0:<.2f}hrs'.format((time.time()-t_int)/60/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "\n",
    "    - 1) Regularize the lstm\n",
    "    - 2) Figure out why there are blanks in the testing\n",
    "    - 3) may consider to collect those left out from the process of creating batches\n",
    "    - 4) related to 3), seq_len = 3 may create null batches \n",
    "\n",
    "    - Split data to defending and offending, as the model for e.g. forward role in deffending and offending should be pretty different. Remove particular events, like free-throw etc.\n",
    "    \n",
    "    - We can use the shot clock as an indicator of when the offending and defending switches.\n",
    "    \n",
    "    - The cameras oprate at 25 frames per second, so in order to learn realistic motions, either we sample the 25 frames, or extend the horizon to 50 for example or even longer(this might be too computationally heavy and model would probably drift a lot).\n",
    "    \n",
    "    - At the moment if we don't have defending or offending sepearted, at least we need to break down the sequences from the 24 secs shot clock, since it usually stands for a change in game state. (note: shot clock sometimes is None)\n",
    "    \n",
    "    - Add tensorboard visualization. Add validation performance (maybe, it would take longer). \n",
    "      tensorboard --logdir=./train_logs\n",
    "      \n",
    "    - Start thinking about 1) joint training 2) Hidden structure 3) Smooth learning\n",
    "    - from each sample to next sample theres not much change, subsample them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Questions\n",
    "\n",
    "    * After a team scored and they go back to get ready for defense, is the going back trajectory pretty much random?\n",
    "    * Do player swap roles during the play? e.g. a forward swapped to a guard, is the forward roles a lot different from gaurds these day? (i.e. can you differentaite a player plaing forward from guard from the game) If yes, then the hidden structure learning/sequencing is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
